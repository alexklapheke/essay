{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "\n",
    "# Feature extraction/transformation libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, Embedding, Input, LSTM, Bidirectional, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Evaluation libraries\n",
    "from eda.accuracy import fuzzy_accuracy\n",
    "\n",
    "# Text-handling libraries\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z\\'-]+')\n",
    "\n",
    "# Other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from progress import show_progress\n",
    "from joblib import Parallel, delayed, dump\n",
    "\n",
    "# Random seed\n",
    "seed = 5777\n",
    "\n",
    "# Options\n",
    "pad_shape = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = pd.read_csv(f\"../data_private/essays_cleaned_target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(essays[\"score\"])\n",
    "\n",
    "# Define vocabulary\n",
    "#vocab = set(tokenizer.tokenize(\" \".join(essays[\"essay\"]))) # using simple regex match\n",
    "vocab = set(token.text for essay in essays[\"essay\"] for token in nlp.tokenizer(essay)) # using spacy's more sophisticated matcher\n",
    "\n",
    "# Convert words to numerical indices <https://www.tensorflow.org/tutorials/text/text_generation>\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(list(vocab))\n",
    "\n",
    "#X_vector = [[word2idx[token] for token in tokenizer.tokenize(essay)] for essay in essays[\"essay\"]] # nltk\n",
    "X_vector = [[word2idx[token.text] for token in nlp.tokenizer(essay)] for essay in essays[\"essay\"]] # spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = [\n",
    "    \"tokens\",\n",
    "    \"types\",\n",
    "    \"sent_len\",\n",
    "    \"word_len\",\n",
    "    \"freq\",\n",
    "    \"semicolons\",\n",
    "    \"link_words\",\n",
    "    \"pps\",\n",
    "    \"max_depth\",\n",
    "]\n",
    "\n",
    "X_meta = essays[meta_cols].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vector_train, \\\n",
    "X_vector_test, \\\n",
    "X_meta_train, \\\n",
    "X_meta_test, \\\n",
    "y_train, \\\n",
    "y_test = train_test_split(X_vector, X_meta, y, random_state=seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays.loc[0, \"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "# Word vectors\n",
    "X_vector_train = pad_sequences(X_vector_train, maxlen=pad_shape)\n",
    "X_vector_test = pad_sequences(X_vector_test, maxlen=pad_shape)\n",
    "\n",
    "# Metadata\n",
    "X_meta_train_sc = ss.fit_transform(X_meta_train)\n",
    "X_meta_test_sc = ss.transform(X_meta_test)\n",
    "\n",
    "pca = PCA(random_state=seed, n_components=5)\n",
    "X_meta_train_pca = pca.fit_transform(X_meta_train_sc)\n",
    "X_meta_test_pca = pca.fit_transform(X_meta_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export train/test data\n",
    "\n",
    "For gridsearching models on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nn-data.p\", \"wb\") as outfile:\n",
    "    pickle.dump((\n",
    "        X_vector_train,\n",
    "        X_vector_test,\n",
    "        X_meta_train_pca,\n",
    "        X_meta_test_pca,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        X_meta_train_pca.shape[1],\n",
    "        len(vocab)\n",
    "    ), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model\n",
    "\n",
    "This RNN has two sets of inputs: the word vectors from the documents, which are put in at the beginning, and the metadata, which is incorporated after the GRU layer ([source](http://digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "82/82 [==============================] - 346s 4s/step - loss: 1.6106 - acc: 0.5717 - val_loss: 1.2525 - val_acc: 0.5824\n",
      "Epoch 2/3\n",
      "82/82 [==============================] - 368s 4s/step - loss: 1.1237 - acc: 0.6068 - val_loss: 1.1125 - val_acc: 0.5767\n",
      "Epoch 3/3\n",
      "82/82 [==============================] - 417s 5s/step - loss: 0.9550 - acc: 0.6762 - val_loss: 1.1304 - val_acc: 0.5732\n"
     ]
    }
   ],
   "source": [
    "def keras_model():\n",
    "    # Borrowed in part from:\n",
    "    # <https://stackoverflow.com/a/55234203>\n",
    "    # <http://digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/>\n",
    "\n",
    "    # Define inputs\n",
    "    vector_input = Input(shape=(pad_shape,))\n",
    "    meta_input = Input(shape=(X_meta_train_pca.shape[1],))\n",
    "\n",
    "    # Define embedding and GRU layers\n",
    "    rnn = Embedding(len(vocab), 96, input_length=pad_shape)(vector_input)\n",
    "    rnn = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(1e-3)))(rnn)\n",
    "    rnn = Bidirectional(GRU(128, return_sequences=False, kernel_regularizer=l2(1e-3)))(rnn)\n",
    "\n",
    "    # Incorporate metadata\n",
    "    rnn = Concatenate()([rnn, meta_input])\n",
    "\n",
    "    # Define hidden and output layers\n",
    "    rnn = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-3))(rnn)\n",
    "    rnn = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-3))(rnn)\n",
    "    rnn = Dense(4, activation=\"softmax\")(rnn)\n",
    "\n",
    "    model = Model(inputs=[vector_input, meta_input], outputs=[rnn])\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "best_score = 0\n",
    "best_model = None\n",
    "summary = \"\"\n",
    "\n",
    "params = {\n",
    "    \"gru_neurons\": [64, 128],\n",
    "    \"l1_neurons\": [64],\n",
    "    \"l2_neurons\": [64],\n",
    "    \"alpha\": [1e-1]\n",
    "}\n",
    "\n",
    "# Fit single model\n",
    "model = keras_model()\n",
    "history = model.fit([X_vector_train, X_meta_train_pca], y_train,\n",
    "                                             validation_data=([X_vector_test, X_meta_test_pca], y_test),\n",
    "                                             epochs=3, verbose=1)\n",
    "\n",
    "# # Hand-rolled gridsearch since sklearn's GridSearchCV doesn't support multiple inputs:\n",
    "# # <https://github.com/keras-team/keras/issues/2748>, <https://github.com/keras-team/keras/issues/9001>\n",
    "# # @show_progress\n",
    "# def keras_gridsearch(p):\n",
    "#     global best_score, best_model, summary\n",
    "#     model = keras_model(*p)\n",
    "#     history = model.fit([X_vector_train, X_meta_train_pca], y_train,\n",
    "#                         validation_data=([X_vector_test, X_meta_test_pca], y_test),\n",
    "#                         epochs=3, verbose=1)\n",
    "#     test_score = history.history[\"val_acc\"][-1]\n",
    "#     summary += str(p) + str(test_score) + \"\\n\"\n",
    "#     if test_score > best_score:\n",
    "#         best_score = test_score\n",
    "#         best_model = model\n",
    "\n",
    "# #keras_gridsearch(itertools.product(*params.values()), update_freq=1)\n",
    "\n",
    "# Parallel(n_jobs=-1, require='sharedmem')(delayed(keras_gridsearch)(p) for p in itertools.product(*params.values()))\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.959954233409611"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict([X_vector_test, X_meta_test_pca])\n",
    "\n",
    "fuzzy_accuracy(\n",
    "    y_test.argmax(axis=1), # Get the vector index with the max value;\n",
    "    y_pred.argmax(axis=1), # i.e., undo the one-hot encoding.\n",
    "    tolerance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 1000)] [(None, 1000)]\n",
      "(None, 1000) (None, 1000, 96)\n",
      "(None, 1000, 96) (None, 1000, 256)\n",
      "(None, 1000, 256) (None, 256)\n",
      "[(None, 5)] [(None, 5)]\n",
      "[(None, 256), (None, 5)] (None, 261)\n",
      "(None, 261) (None, 128)\n",
      "(None, 128) (None, 128)\n",
      "(None, 128) (None, 4)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.input_shape, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(vocab, \"../EssayScorer/vocab.bin\", compress=True)\n",
    "dump(ss, \"../EssayScorer/scaler.bin\", compress=True)\n",
    "dump(pca, \"../EssayScorer/pca.bin\", compress=True)\n",
    "model.save(\"../EssayScorer/model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.620137299771167"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"../EssayScorer/model.keras\")\n",
    "y_pred = model.predict([X_vector_test, X_meta_test_pca])\n",
    "\n",
    "fuzzy_accuracy(\n",
    "    y_test.argmax(axis=1), # Get the vector index with the max value;\n",
    "    y_pred.argmax(axis=1), # i.e., undo the one-hot encoding.\n",
    "    tolerance=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326.85px",
    "left": "1351px",
    "right": "20px",
    "top": "120px",
    "width": "336.5px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
