{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-handling libraries\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "\n",
    "# Feature extraction/transformation libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, Embedding, Input, LSTM, Bidirectional, Concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Text-handling libraries\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[A-Za-z\\'-]+')\n",
    "\n",
    "# Other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from progress import show_progress\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Random seed\n",
    "seed = 5777\n",
    "\n",
    "# Options\n",
    "pad_shape = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = pd.read_csv(f\"../data_private/essays_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_set = essays[(essays[\"essay_set\"] == 3) | (essays[\"essay_set\"] == 4)]\n",
    "\n",
    "y = to_categorical(target_set[\"score\"])\n",
    "\n",
    "# Define vocabulary\n",
    "#vocab = set(tokenizer.tokenize(\" \".join(target_set[\"essay\"]))) # using simple regex match\n",
    "vocab = set(token.text for essay in target_set[\"essay\"] for token in nlp.tokenizer(essay)) # using spacy's more sophisticated matcher\n",
    "\n",
    "# Convert words to numerical indices <https://www.tensorflow.org/tutorials/text/text_generation>\n",
    "word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2word = np.array(list(vocab))\n",
    "\n",
    "#X_vector = [[word2idx[token] for token in tokenizer.tokenize(essay)] for essay in target_set[\"essay\"]] # nltk\n",
    "X = [[word2idx[token.text] for token in nlp.tokenizer(essay)] for essay in target_set[\"essay\"]] # spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(target_set[\"types\"], target_set[\"score\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>types</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4053</th>\n",
       "      <td>It think  it means  to be all you can be.</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4318</th>\n",
       "      <td>The setting effects the cyclist very much. By ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4825</th>\n",
       "      <td>The features of the setting affected the cycli...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5930</th>\n",
       "      <td>Saeng would return to her homeland.</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>So she saying she will take the test next year.</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6280</th>\n",
       "      <td>He  wrote this  because  he is talking  about ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6313</th>\n",
       "      <td>Reserved need to check keenly</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6444</th>\n",
       "      <td>NO IMAGE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>@PERSON1 go to the top and then over the hill.</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>I think they think it will bring them luck</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay  types  score\n",
       "4053          It think  it means  to be all you can be.    7.0    0.0\n",
       "4318  The setting effects the cyclist very much. By ...    9.0    2.0\n",
       "4825  The features of the setting affected the cycli...    8.0    1.0\n",
       "5930                Saeng would return to her homeland.    6.0    0.0\n",
       "6199    So she saying she will take the test next year.    9.0    1.0\n",
       "6280  He  wrote this  because  he is talking  about ...    9.0    0.0\n",
       "6313                      Reserved need to check keenly    5.0    3.0\n",
       "6444                                           NO IMAGE    2.0    1.0\n",
       "6726     @PERSON1 go to the top and then over the hill.    9.0    0.0\n",
       "6989         I think they think it will bring them luck    5.0    1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_set[target_set[\"types\"] < 10][[\"essay\", \"types\", \"score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_id                                   9870\n",
       "essay_set                                     4\n",
       "essay             Reserved need to check keenly\n",
       "rater1_domain1                                3\n",
       "rater2_domain1                                3\n",
       "                              ...              \n",
       "semicolons                                    0\n",
       "link_words                                    0\n",
       "pps                                           5\n",
       "max_depth                                     2\n",
       "score                                         3\n",
       "Name: 6313, Length: 112, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_set.loc[6313, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = [\n",
    "    \"tokens\",\n",
    "    \"types\",\n",
    "    \"sent_len\",\n",
    "    \"word_len\",\n",
    "    \"freq\",\n",
    "    \"semicolons\",\n",
    "    \"link_words\",\n",
    "    \"pps\",\n",
    "    \"max_depth\"\n",
    "]\n",
    "\n",
    "X_meta = target_set[meta_cols].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vector_train, \\\n",
    "X_vector_test, \\\n",
    "X_meta_train, \\\n",
    "X_meta_test, \\\n",
    "y_train, \\\n",
    "y_test = train_test_split(X_vector, X_meta, y, random_state=seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "# Word vectors\n",
    "X_vector_train = pad_sequences(X_vector_train, maxlen=pad_shape)\n",
    "X_vector_test = pad_sequences(X_vector_test, maxlen=pad_shape)\n",
    "\n",
    "# Metadata\n",
    "X_meta_train_sc = ss.fit_transform(X_meta_train)\n",
    "X_meta_test_sc = ss.transform(X_meta_test)\n",
    "\n",
    "pca = PCA(random_state=seed, n_components=5)\n",
    "X_meta_train_pca = pca.fit_transform(X_meta_train_sc)\n",
    "X_meta_test_pca = pca.fit_transform(X_meta_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export train/test data\n",
    "\n",
    "For gridsearching models on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nn-data.p\", \"wb\") as outfile:\n",
    "    pickle.dump((\n",
    "        X_vector_train,\n",
    "        X_vector_test,\n",
    "        X_meta_train_pca,\n",
    "        X_meta_test_pca,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        X_meta_train_pca.shape[1],\n",
    "        len(vocab)\n",
    "    ), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta_train_pca.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "This RNN has two sets of inputs: the word vectors from the documents, which are put in at the beginning, and the metadata, which is incorporated after the GRU layer ([source](http://digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "82/82 [==============================] - 167s 2s/step - loss: 4.5747 - acc: 0.5454 - val_loss: 2.2762 - val_acc: 0.6133\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 161s 2s/step - loss: 1.6185 - acc: 0.6171 - val_loss: 1.2870 - val_acc: 0.6373\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 148s 2s/step - loss: 1.1599 - acc: 0.6205 - val_loss: 1.1264 - val_acc: 0.6281\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 147s 2s/step - loss: 1.0607 - acc: 0.6240 - val_loss: 1.0554 - val_acc: 0.6362\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 145s 2s/step - loss: 1.0140 - acc: 0.6224 - val_loss: 1.0171 - val_acc: 0.6522\n"
     ]
    }
   ],
   "source": [
    "def keras_model(gru_neurons, l1_neurons, l2_neurons, alpha):\n",
    "    # Borrowed in part from:\n",
    "    # <https://stackoverflow.com/a/55234203>\n",
    "    # <http://digital-thinking.de/deep-learning-combining-numerical-and-text-features-in-deep-neural-networks/>\n",
    "\n",
    "    # Define inputs\n",
    "    vector_input = Input(shape=(pad_shape,))\n",
    "    meta_input = Input(shape=(X_meta_train_pca.shape[1],))\n",
    "\n",
    "    # Define embedding and GRU layers\n",
    "    rnn = Embedding(len(vocab), 96, input_length=pad_shape)(vector_input)\n",
    "    rnn = Bidirectional(GRU(gru_neurons, return_sequences=True, kernel_regularizer=l2(0.01)))(rnn)\n",
    "    rnn = Bidirectional(GRU(gru_neurons, return_sequences=False, kernel_regularizer=l2(0.01)))(rnn)\n",
    "\n",
    "    # Incorporate metadata\n",
    "    rnn = Concatenate()([rnn, meta_input])\n",
    "\n",
    "    # Define hidden and output layers\n",
    "    rnn = Dense(l1_neurons, activation=\"relu\", kernel_regularizer=l2(0.01))(rnn)\n",
    "    rnn = Dense(l2_neurons, activation=\"relu\", kernel_regularizer=l2(0.01))(rnn)\n",
    "    rnn = Dense(4, activation=\"softmax\")(rnn)\n",
    "\n",
    "    model = Model(inputs=[vector_input, meta_input], outputs=[rnn])\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "best_score = 0\n",
    "best_model = None\n",
    "summary = \"\"\n",
    "\n",
    "params = {\n",
    "    \"gru_neurons\": [64, 128],\n",
    "    \"l1_neurons\": [64],\n",
    "    \"l2_neurons\": [64],\n",
    "    \"alpha\": [1e-1]\n",
    "}\n",
    "\n",
    "# Fit single model\n",
    "model = keras_model(64, 64, 64, 1e-1)\n",
    "history = model.fit([X_vector_train, X_meta_train_pca], y_train,\n",
    "                                             validation_data=([X_vector_test, X_meta_test_pca], y_test),\n",
    "                                             epochs=5, verbose=1)\n",
    "\n",
    "# # Hand-rolled gridsearch\n",
    "# # @show_progress\n",
    "# def keras_gridsearch(p):\n",
    "#     global best_score, best_model, summary\n",
    "#     model = keras_model(*p)\n",
    "#     history = model.fit([X_vector_train, X_meta_train_pca], y_train,\n",
    "#                         validation_data=([X_vector_test, X_meta_test_pca], y_test),\n",
    "#                         epochs=3, verbose=1)\n",
    "#     test_score = history.history[\"val_acc\"][-1]\n",
    "#     summary += str(p) + str(test_score) + \"\\n\"\n",
    "#     if test_score > best_score:\n",
    "#         best_score = test_score\n",
    "#         best_model = model\n",
    "\n",
    "# #keras_gridsearch(itertools.product(*params.values()), update_freq=1)\n",
    "\n",
    "# Parallel(n_jobs=-1, require='sharedmem')(delayed(keras_gridsearch)(p) for p in itertools.product(*params.values()))\n",
    "\n",
    "# print(summary)\n",
    "\n",
    "# # Save winning model to disk\n",
    "model.save(\"../model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_accuracy(y_true, y_pred, tolerance):\n",
    "    \"\"\"Returns accuracy of a model trained on numeric data with a tolerance.\n",
    "       For example, with a tolerance of 1, a model prediction of 9 for a true\n",
    "       value of 10 will be counted in the \"fuzzy accuracy\".\"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return np.mean(np.abs(y_true - y_pred) <= tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_vector_test, X_meta_test_pca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 2, 2, 1, 1, 3, 2, 3, 1, 2, 1, 3, 1, 1, 2, 2, 1, 1, 2, 2, 3,\n",
       "       3, 1, 2, 1, 2, 3, 3, 1, 2, 3, 2, 2, 1, 1, 2, 1, 1, 3, 2, 3, 1, 2,\n",
       "       2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 3, 3, 1, 2, 3, 2, 1, 3,\n",
       "       2, 2, 3, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2,\n",
       "       1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 3, 2, 2, 3, 2, 1, 1, 3, 1, 2, 2, 1,\n",
       "       2, 2, 3, 1, 1, 1, 1, 3, 3, 1, 2, 1, 2, 3, 1, 2, 2, 1, 1, 2, 2, 1,\n",
       "       1, 2, 2, 1, 1, 1, 2, 3, 2, 2, 1, 2, 2, 1, 3, 1, 1, 1, 2, 1, 2, 1,\n",
       "       1, 3, 2, 1, 2, 1, 2, 1, 3, 2, 2, 2, 2, 1, 3, 3, 1, 1, 2, 1, 2, 2,\n",
       "       1, 3, 3, 1, 1, 3, 3, 2, 3, 2, 2, 3, 2, 2, 2, 1, 1, 2, 1, 1, 2, 2,\n",
       "       1, 2, 1, 2, 1, 3, 1, 2, 2, 3, 3, 1, 3, 3, 3, 2, 3, 2, 3, 1, 1, 1,\n",
       "       1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 3, 2, 3, 2, 1, 2, 2, 1,\n",
       "       2, 1, 3, 1, 1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 2, 2, 1, 3, 3, 2, 1, 3,\n",
       "       1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 3, 2, 3, 3, 2, 3, 1, 2, 2,\n",
       "       2, 1, 2, 1, 3, 2, 2, 1, 2, 2, 2, 3, 2, 1, 1, 2, 1, 2, 2, 3, 1, 2,\n",
       "       2, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 3, 3, 2, 3, 1,\n",
       "       2, 2, 2, 3, 1, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2, 2, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 2, 1, 1, 2, 1, 3, 1, 2, 2, 3, 1, 2, 1, 1, 3, 1, 2, 1, 3, 3,\n",
       "       2, 2, 1, 1, 1, 1, 3, 1, 1, 2, 3, 3, 3, 2, 2, 2, 1, 2, 1, 1, 1, 1,\n",
       "       1, 2, 1, 3, 1, 2, 1, 3, 2, 3, 1, 3, 3, 3, 2, 2, 1, 2, 2, 2, 1, 3,\n",
       "       2, 1, 1, 1, 3, 1, 2, 1, 2, 3, 2, 2, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3,\n",
       "       2, 1, 1, 3, 2, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 2,\n",
       "       2, 3, 3, 1, 3, 1, 1, 2, 2, 1, 1, 2, 3, 1, 2, 2, 1, 1, 3, 2, 2, 3,\n",
       "       1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 1, 2, 2,\n",
       "       1, 2, 1, 1, 1, 1, 3, 1, 2, 3, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1,\n",
       "       1, 2, 2, 1, 1, 1, 2, 2, 1, 3, 1, 3, 2, 2, 2, 2, 1, 1, 1, 3, 2, 2,\n",
       "       2, 2, 1, 2, 2, 1, 3, 3, 2, 2, 2, 3, 1, 1, 2, 2, 1, 2, 1, 3, 1, 2,\n",
       "       1, 2, 2, 2, 2, 2, 3, 2, 2, 1, 1, 1, 1, 1, 3, 3, 2, 1, 2, 2, 1, 2,\n",
       "       1, 2, 3, 3, 1, 2, 2, 1, 2, 1, 1, 2, 2, 3, 2, 2, 1, 2, 1, 1, 1, 3,\n",
       "       1, 1, 3, 2, 2, 2, 2, 2, 3, 2, 1, 1, 2, 3, 2, 3, 1, 2, 1, 2, 1, 2,\n",
       "       2, 2, 3, 3, 1, 2, 3, 1, 3, 1, 3, 1, 2, 1, 2, 1, 1, 2, 1, 3, 3, 3,\n",
       "       1, 2, 3, 2, 2, 1, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 1, 1, 1, 3, 2, 3, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3,\n",
       "       3, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 1, 1, 1, 1,\n",
       "       3, 1, 1, 1, 3, 1, 1, 1, 3, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 3,\n",
       "       3, 2, 3, 1, 2, 3, 1, 3, 2, 2, 2, 3, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1,\n",
       "       1, 3, 3, 1, 1, 2, 1, 3, 2, 1, 1, 1, 2, 3, 2, 1, 3, 1, 1, 3, 1, 2,\n",
       "       3, 2, 3, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 3, 2, 2, 3, 1, 2, 1, 2, 1,\n",
       "       2, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 2, 2, 1, 2, 2, 3, 1, 1, 3, 2, 1,\n",
       "       3, 1, 2, 2, 2, 1, 2, 1, 3, 1, 1, 2, 3, 2, 2, 3, 1, 2, 1, 1, 1, 1,\n",
       "       3, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_accuracy(y_test.argmax(axis=1), y_pred.argmax(axis=1), tolerance=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "326.85px",
    "left": "1351px",
    "right": "20px",
    "top": "120px",
    "width": "336.5px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
