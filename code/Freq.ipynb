{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compiles an English frequency list from the 15-million-word, genre-balanced [American National Corpus](http://www.anc.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import glob\n",
    "import spacy\n",
    "import re\n",
    "from unidecode import unidecode # strip diacritics\n",
    "from progress import show_progress\n",
    "\n",
    "# Options\n",
    "anc_path = \"/home/alex/Data/ANC/\" # freely downloadable from anc.org\n",
    "dict_path = \"/usr/share/dict/words\" # wamerican-insane v2017.08.24-1, which contains 654,749 entries\n",
    "freq_per = 100_000 # scaling factor (i.e., compute frequency per this many words)\n",
    "include_hapaxes = True\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Parsed 8824 items in 37:47.\n",
      "13265231 tokens, 104480 types\n"
     ]
    }
   ],
   "source": [
    "freqs = {}\n",
    "total_tokens = 0\n",
    "\n",
    "with open(dict_path, \"r\") as file:\n",
    "    dictionary = set(file.read().split(\"\\n\"))\n",
    "\n",
    "@show_progress\n",
    "def parse_files_into_tokens(i, filename):\n",
    "    global total_tokens\n",
    "    \n",
    "    # Open each file in the corpus\n",
    "    with open(filename, \"r\") as file:\n",
    "        \n",
    "        # Remove diacritics, parse, & tokenize\n",
    "        for token in nlp(unidecode(file.read())):\n",
    "            \n",
    "            # Eliminate non-words\n",
    "            if not token.is_punct and not token.is_space:\n",
    "                \n",
    "                # Lemmatize and remove diacritics/ligatures\n",
    "                lemma = token.lemma_.lower().strip(\"-\")\n",
    "                \n",
    "                # Only use dictionary words\n",
    "                if lemma in dictionary:\n",
    "                \n",
    "                    # Add lemma/part-of-speech tag\n",
    "                    type_pos = \",\".join([lemma, token.pos_])\n",
    "                    \n",
    "                    # Update our dictionary\n",
    "                    freqs[type_pos] = freqs.setdefault(type_pos, 0) + 1\n",
    "\n",
    "                    # Update our running total\n",
    "                    total_tokens += 1\n",
    "\n",
    "parse_files_into_tokens(\n",
    "    glob.iglob(anc_path + \"**/*.txt\", recursive=True), # Get all text files recursively <https://stackoverflow.com/a/45172387>\n",
    "    update_freq = 1 # Update countdown timer after every file\n",
    ")\n",
    "\n",
    "print(f\"{total_tokens} tokens, {len(freqs.keys())} types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_sorted = dict(sorted(freqs.items())) # <https://stackoverflow.com/a/9001529>\n",
    "\n",
    "with open(\"anc_frequency_list.csv\", \"w\") as file:\n",
    "\n",
    "    # CSV header\n",
    "    file.write(f\"lemma,pos,freq_per_{freq_per}\\n\")\n",
    "\n",
    "    # CSV rows\n",
    "    for word, freq in freqs_sorted.items():\n",
    "        if include_hapaxes or freq > 1:\n",
    "            file.write(f\"{word},{freq_per*freq/total_tokens}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "524.85px",
    "left": "1373px",
    "right": "20px",
    "top": "120px",
    "width": "314.5px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
